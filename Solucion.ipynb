{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Práctica de procesamiento de lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inteligencia Artificial\n",
    "### Grado en Ingeniería Informática - Ingeniería del Software\n",
    "### Universidad de Sevilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Los sistemas de texto predictivo son una tecnología de entrada de texto diseñada para dispositivos móviles. Esta tecnología permite formar palabras presionando una zona de la pantalla asociada a un grupo de letras. La aplicación principal de esta tecnología es simplificar la escritura de mensajes de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque esta tecnología se utilizó inicialmente para facilitar la escritura de mensajes en teléfonos móviles con teclado numérico, la aparición del smartphone con teclado ampliado provocó que cambiase el tipo de aplicaciones a las que se aplica. Actualmente se utiliza tanto para facilitar la escritura en teclados ampliados (samsung swype) como para sugerir nuevas entradas de texto (escritura inteligente). También es notable su uso en dispositivos móviles más pequeños como los Smart Watch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la pantalla de un Smart Watch, un teclado podría consistir en agrupar las letras del alfabeto de la siguiente manera:\n",
    "\n",
    "<img src=\"teclado-touchone.png\"/>\n",
    "\n",
    "Esta es una imagen del teclado de la aplicación TouchOne de 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para escribir texto con este teclado hay que pulsar las zonas asociadas a cada letra para componer las palabras. Por ejemplo, para escribir la palabra *Hola* se debería:\n",
    "\n",
    "* Pulsar la esquina superior derecha (h).\n",
    "* Pulsar el lateral derecho (o).\n",
    "* Pulsar el lateral izquierdo (l).\n",
    "* Pulsar la esquina superior izquierda (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los sistemas de texto predicen la palabra que queremos escribir a partir de las pulsaciones realizadas. Para ello han efectuado previamente un análisis estadístico de un corpus de textos de referencia, determinando las probabilidades de las correspondencias entre distintas secuencias de pulsaciones y posibles palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos sistemas suelen mostrar la palabra que corresponde con mayor probabilidad a la combinación de pulsaciones realizada por el usuario, actualizándose esta palabra a medida que el usuario realiza nuevas pulsaciones. Además, es habitual ofrecer al usuario la posibilidad de requerir otras posibilidades, aparte de aceptar la palabra propuesta por el sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La efectividad de un sistema de prediccion de texto dependerá de varios factores:\n",
    "\n",
    "* La calidad del corpus.\n",
    "* El modelo de lenguaje obtenido a partir del análisis estadístico del corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta práctica se propone la implementación de un sistema de predicción de texto para un teclado similar al de la aplicación TouchOne mostrada antes. Cada bloque de letras tendrá asociado un número del 1 al 8 de la siguiente forma:\n",
    "1. a, á, b, c\n",
    "2. d, e, é, f\n",
    "3. g, h, i, í\n",
    "4. j, k, l\n",
    "5. m, n, ñ, o, ó\n",
    "6. p, q, r, s\n",
    "7. t, u, ú, ü, v\n",
    "8. w, x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma, si por ejemplo se pretendiera escribir la frase `algoritmo de texto predictivo`, la entrada al sistema sería la secuencia de números (y espacios en blanco) `143563755 22 72875 6622317375`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloques_de_letras = {'1': 'aábc', '2':'deéf', '3': 'ghií', '4': 'jkl',\n",
    "                     '5': 'mnñoó', '6': 'pqrs', '7': 'tuúüv', '8': 'wxyz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codifica_palabra(palabra):\n",
    "    códigos = []\n",
    "    for letra in palabra:\n",
    "        for código, bloque in bloques_de_letras.items():\n",
    "            if letra in bloque:\n",
    "                códigos.append(código)\n",
    "                break\n",
    "    return ''.join(códigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'143563755'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codifica_palabra('algoritmo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La plataforma NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la realización de esta práctica nos apoyaremos en [NLTK](https://www.nltk.org), que es una plataforma de Python que proporciona un conjunto de bibliotecas para el procesamiento del lenguaje natural, junto con interfaces fáciles de usar a multitud de corpus y recursos léxicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a ilustrar con un ejemplo sencillo el uso de NLTK para la construcción de [modelos de lenguajes basados en n-gramas](https://www.nltk.org/api/nltk.lm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar establecemos cuál será nuestro vocabulario de términos, en este caso formado por las letras a, b y c y los símbolos de inicio y fin de frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.vocabulary import Vocabulary\n",
    "\n",
    "vocabulario = Vocabulary(['a', 'b', 'c', '<s>', '</s>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar todos los términos desconocidos que nos podamos encontrar en un texto se añade automáticamente el término `'<UNK>'` (de *unknown*, desconocido en inglés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'<UNK>' in vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para buscar un término en el vocabulario basta usar el método `lookup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "print(vocabulario.lookup('a'))\n",
    "print(vocabulario.lookup('e'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a establecer un corpus de entrenamiento y un corpus de prueba. En NLTK las «frases» deben ser listas de términos y los corpus listas de frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_entrenamiento = [['a', 'b', 'b', 'b'], ['d', 'a', 'b'], ['a', 'b', 'a', 'e']]\n",
    "corpus_prueba = [['a', 'b', 'a', 'b'], ['a', 'b']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para determinar los posibles n-gramas de una frase se puede utilizar la función `ngrams`. Existen también las funciones `bigrams` y `trigrams` que, como indican sus nombres, permiten determinar los bigramas y trigramas de una frase. Finalmente, la función `everygrams` determina todos los posibles n-gramas hasta el orden especificado.\n",
    "\n",
    "*Nota*: estas funciones devuelven [iteradores](https://docs.python.org/es/3/tutorial/classes.html#iterators), que básicamente son secuencias *perezosas*, en las que sus elementos no se determinan hasta que son requeridos, evitando de esta forma tener que almacenarlos a todos en memoria. Es por ello que para poder observar los resultados de los ejemplos siguientes se construyen listas con los elementos de los iteradores obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a',), ('b',), ('a',), ('e',)]\n",
      "[('a', 'b'), ('b', 'a'), ('a', 'e')]\n",
      "[('a', 'b', 'a'), ('b', 'a', 'e')]\n",
      "[('a', 'b'), ('a', 'b', 'a'), ('b', 'a'), ('b', 'a', 'e'), ('a', 'e')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams, bigrams, trigrams, everygrams\n",
    "\n",
    "print(list(ngrams(['a', 'b', 'a', 'e'], n=1)))\n",
    "print(list(bigrams(['a', 'b', 'a', 'e'])))\n",
    "print(list(trigrams(['a', 'b', 'a', 'e'])))\n",
    "print(list(everygrams(['a', 'b', 'a', 'e'], min_len=2, max_len=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de los n-gramas de orden mayor que 1, es conveniente delimitar las frases con los símbolos de inicio y fin de frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'a', 'b', 'a', 'e', '</s>', '</s>']\n",
      "[('<s>', '<s>'), ('<s>', '<s>', 'a'), ('<s>', 'a'), ('<s>', 'a', 'b'), ('a', 'b'), ('a', 'b', 'a'), ('b', 'a'), ('b', 'a', 'e'), ('a', 'e'), ('a', 'e', '</s>'), ('e', '</s>'), ('e', '</s>', '</s>'), ('</s>', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "\n",
    "print(list(pad_sequence(['a', 'b', 'a', 'e'], n=3,  # la secuencia se delimita con n-1 símbolos\n",
    "                        pad_left=True, left_pad_symbol='<s>',\n",
    "                        pad_right=True, right_pad_symbol='</s>')))\n",
    "print(list(everygrams(['a', 'b', 'a', 'e'], min_len=2, max_len=3,\n",
    "                      pad_left=True, left_pad_symbol='<s>',\n",
    "                      pad_right=True, right_pad_symbol='</s>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estimar, mediante máxima verosimilitud, la probabilidad de los distintos n-gramas, basta instanciar la clase `MLE`. Hay que proporcionar el máximo orden de los n-gramas a considerar y, opcionalmente, el vocabulario de términos. En caso de que este último no se proporcione, también se podrá inferir a partir de los términos que aparezcan en un corpus de texto proporcionado al entrenar, mediante el método `fit`, el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "modelo_lenguaje = MLE(3, vocabulary=vocabulario)\n",
    "ngramas = (everygrams(frase, min_len=2, max_len=3,\n",
    "                      pad_left=True, left_pad_symbol='<s>',\n",
    "                      pad_right=True, right_pad_symbol='</s>')\n",
    "           for frase in corpus_entrenamiento)\n",
    "modelo_lenguaje.fit(ngramas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las probabilidades estimadas de los unigramas son todas iguales a 0, ya que no hemos proporcionado estos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'b': 0, 'c': 0, '<s>': 0, '</s>': 0, '<UNK>': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{t: modelo_lenguaje.score(t) for t in vocabulario}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No ocurre lo mismo para los bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'a'): 0.0,\n",
       " ('a', 'b'): 0.75,\n",
       " ('a', 'c'): 0.0,\n",
       " ('a', '<s>'): 0.0,\n",
       " ('a', '</s>'): 0.0,\n",
       " ('a', '<UNK>'): 0.25}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{('a', t): modelo_lenguaje.score(t, context=['a']) for t in vocabulario}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tampoco para los trigramas. En este caso mostramos el logaritmo de la probabilidad estimada, que es lo que se debe usar en productos de probabilidades para evitar desbordamientos numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'b', 'a'): -1.5849625007211563,\n",
       " ('a', 'b', 'b'): -1.5849625007211563,\n",
       " ('a', 'b', 'c'): -inf,\n",
       " ('a', 'b', '<s>'): -inf,\n",
       " ('a', 'b', '</s>'): -1.5849625007211563,\n",
       " ('a', 'b', '<UNK>'): -inf}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{('a', 'b', t): modelo_lenguaje.logscore(t, context=['a', 'b']) for t in vocabulario}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, calculamos la perplejidad del modelo sobre el corpus de prueba. Para ello debemos proporcionar una secuencia plana de los n-gramas a evaluar, para lo que es útil la función `flatten`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.254180002028708"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "\n",
    "ngramas = (bigrams(frase,\n",
    "                   pad_left=True, left_pad_symbol='<s>',\n",
    "                   pad_right=True, right_pad_symbol='</s>')\n",
    "           for frase in corpus_prueba)\n",
    "modelo_lenguaje.perplexity(flatten(ngramas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de texto a partir de modelos de $n$-gramas de letras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta primera parte de la práctica se pretende construir un sistema de predicción de texto basado en un modelo de lenguaje construido a partir de modelos de $n$-gramas de letras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar se debe cargar un corpus de textos en español que nos permita entrenar el modelo. Se puede, por ejemplo, utilizar el corpus proporcionado en el fichero `corpus.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK proporciona diferentes clases que permiten leer corpus de muy diversos tipos. Entre ellas se encuentra la clase `PlaintextCorpusReader` para leer corpus proporcionados en ficheros de texto plano, como es el caso que nos ocupa. Esta clase identificará las frases y palabras contenidas en el corpus, en un proceso conocido como *tokenización* (es decir, identificación de los *tokens* que componen el texto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK es suficientemente flexible como para permitirnos elegir los algoritmos que separan el texto en párrafos, frases y palabras, pero nosotros usaremos los algoritmos por defecto: las líneas en blanco separan los párrafos; las frases se separan en palabras formadas bien por secuencias de caracteres alfanuméricos, bien por secuencias de caracteres no alfanuméricos ni espacios (que se descartan); los párrafos se separan en frases usando un modelo entrenado mediante aprendizaje no supervisado que se ha comprobado que funciona bien para muchos lenguajes europeos.\n",
    "\n",
    "Para poder usar este último modelo es necesario descargarlo primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('punkt', download_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya podemos leer el corpus de entrenamiento (que, aunque no es el caso, podría estar dividido en varios ficheros de texto), estableciendo como identificador de frases el modelo entrenado para el español. Establecemos también explícitamente [UTF-8](https://es.wikipedia.org/wiki/UTF-8) como codificación de caracteres para evitar problemas en sistemas Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.data import load\n",
    "\n",
    "corpus_entrenamiento = PlaintextCorpusReader(root='.', fileids=['corpus.txt'], encoding='utf8',\n",
    "                                             sent_tokenizer=load('tokenizers/punkt/spanish.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `sents` proporciona las frases identificadas en el corpus (como devuelve un iterador, hacemos uso de la biblioteca estándar *itertools* para manejarlo con comodidad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¿', 'Quién', 'o', 'qué', 'mutila', 'y', 'mata', 'a', 'los', 'niños', 'de', 'un', 'pequeño', 'pueblo', 'norteamericano', '?']\n",
      "['¿', 'Por', 'qué', 'llega', 'cíclicamente', 'el', 'horror', 'a', 'Derry', 'en', 'forma', 'de', 'un', 'payaso', 'siniestro', 'que', 'va', 'sembrando', 'la', 'destrucción', 'a', 'su', 'paso', '?']\n",
      "['Esto', 'es', 'lo', 'que', 'se', 'proponen', 'averiguar', 'los', 'protagonistas', 'de', 'esta', 'novela', '.']\n",
      "['Tras', 'veintisiete', 'años', 'de', 'tranquilidad', 'y', 'lejanía', 'una', 'antigua', 'promesa', 'infantil', 'les', 'hace', 'volver', 'al', 'lugar', 'en', 'el', 'que', 'vivieron', 'su', 'infancia', 'y', 'juventud', 'como', 'una', 'terrible', 'pesadilla', '.']\n",
      "['Regresan', 'a', 'Derry', 'para', 'enfrentarse', 'con', 'su', 'pasado', 'y', 'enterrar', 'definitivamente', 'la', 'amenaza', 'que', 'los', 'amargó', 'durante', 'su', 'niñez', '.']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for frase in itertools.islice(corpus_entrenamiento.sents(), 5):\n",
    "    print(frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma análoga, el método `words` proporciona las palabras identificadas en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿\n",
      "Quién\n",
      "o\n",
      "qué\n",
      "mutila\n",
      "y\n",
      "mata\n",
      "a\n",
      "los\n",
      "niños\n"
     ]
    }
   ],
   "source": [
    "for palabra in itertools.islice(corpus_entrenamiento.words(), 10):\n",
    "    print(palabra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para separar una palabra en la lista de sus caracteres basta usar la función `list` estándar de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'o', 'r', 't', 'e', 'a', 'm', 'e', 'r', 'i', 'c', 'a', 'n', 'o']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('norteamericano')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a la descripción del sistema de predicción de texto que se quiere construir, el vocabulario estará formado por las letras del alfabeto español, junto con los símbolos de inicio y fin de palabra (en esta parte de la práctica las secuencias que manejamos son las distintas palabras del corpus) y el símbolo de letra desconocida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'á', 'b', 'c', 'd', 'e', 'é', 'f', 'g', 'h', 'i', 'í', 'j', 'k', 'l', 'm', 'n', 'ñ', 'o', 'ó', 'p', 'q', 'r', 's', 't', 'u', 'ú', 'ü', 'v', 'w', 'x', 'y', 'z', '<s>', '</s>', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "letras = [letra for bloque in bloques_de_letras.values() for letra in bloque]\n",
    "inicio_palabra = '<s>'\n",
    "fin_palabra = '</s>'\n",
    "vocabulario_letras = Vocabulary(letras + [inicio_palabra, fin_palabra])\n",
    "print(list(vocabulario_letras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 1**: construir un modelo de unigramas de letras entrenado con el corpus proporcionado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: para este modelo no es necesario delimitar las palabras con los símbolos especiales de inicio y fin ya que únicamente introducirán un factor multiplicativo al estimar la probabilidad de las distintas palabras, por lo que no afectará a la comparación entre esas probabilidades que, como veremos, es lo que se buscará realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_unigrama_letras = MLE(1, vocabulary=vocabulario_letras)\n",
    "unigramas_letras = (ngrams(list(palabra), n=1)\n",
    "                    for palabra in corpus_entrenamiento.words())\n",
    "modelo_unigrama_letras.fit(unigramas_letras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez construido el modelo de lenguaje, podemos seguir tres estrategias a la hora de predecir qué palabra ha tratado de introducir el usuario a partir de la secuencia de pulsaciones realizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 2**: definir una función que reciba una cadena de pulsaciones de teclas y devuelva una cadena construida con las letras más probables correspondientes a esas pulsaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letras_más_probables(pulsaciones):\n",
    "    letras = []\n",
    "    for pulsación in pulsaciones:\n",
    "        bloque = bloques_de_letras[pulsación]\n",
    "        letra_más_probable = max(bloque, key=modelo_unigrama_letras.score)\n",
    "        letras.append(letra_más_probable)\n",
    "    return ''.join(letras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'isito'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letras_más_probables(codifica_palabra('grito'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 3**: definir una función que reciba una cadena de pulsaciones de teclas y devuelva la cadena más probable compatible con esas pulsaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ayuda: considérese el uso de la función `itertools.product` para determinar todas las cadenas compatibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabra_más_probable(pulsaciones):\n",
    "    palabra_más_probable = ''\n",
    "    mayor_log_probabilidad = float('-inf')\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        log_probabilidad_palabra = sum(modelo_unigrama_letras.logscore(letra)\n",
    "                                       for letra in palabra)\n",
    "        if log_probabilidad_palabra > mayor_log_probabilidad:\n",
    "            palabra_más_probable = palabra\n",
    "            mayor_log_probabilidad = log_probabilidad_palabra\n",
    "    return ''.join(palabra_más_probable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'isito'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabra_más_probable(codifica_palabra('grito'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 4**: definir una función que reciba una cadena de pulsaciones de teclas y una cantidad $n$ de palabras y devuelva las $n$ cadenas más probables compatibles con esas pulsaciones, en orden decreciente de probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabras_más_probables(pulsaciones, n):\n",
    "    palabras = {}\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        log_probabilidad_palabra = sum(modelo_unigrama_letras.logscore(letra)\n",
    "                                       for letra in palabra)\n",
    "        palabras[''.join(palabra)] = log_probabilidad_palabra\n",
    "    return sorted(palabras, key=palabras.get, reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['isito', 'irito', 'isiuo', 'iriuo', 'isitn']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras_más_probables(codifica_palabra('grito'), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo unigrama de letras no proporciona resultados satisfactorios, ya que no utiliza la información del contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 5**: repite los ejercicios 1 a 4 anteriores, pero construyendo modelos bigramas y trigramas de letras, en lugar de un modelo unigrama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hrito\n",
      "grito\n",
      "['grito', 'grgun', 'irito', 'isito', 'grivo']\n"
     ]
    }
   ],
   "source": [
    "modelo_bigrama_letras = MLE(2, vocabulary=vocabulario_letras)\n",
    "bigramas_letras = (bigrams(list(palabra),\n",
    "                           pad_left=True, left_pad_symbol='<s>',\n",
    "                           pad_right=True, right_pad_symbol='</s>')\n",
    "                   for palabra in corpus_entrenamiento.words())\n",
    "modelo_bigrama_letras.fit(bigramas_letras)\n",
    "\n",
    "def letras_más_probables(pulsaciones):\n",
    "    letras = []\n",
    "    letra_anterior = '<s>'\n",
    "    for pulsación in pulsaciones:\n",
    "        bloque = bloques_de_letras[pulsación]\n",
    "        letra_más_probable = max(bloque,\n",
    "                                 key=lambda letra: modelo_bigrama_letras.score(letra, context=[letra_anterior]))\n",
    "        letras.append(letra_más_probable)\n",
    "        letra_anterior = letra_más_probable\n",
    "    return ''.join(letras)\n",
    "\n",
    "print(letras_más_probables(codifica_palabra('grito')))\n",
    "\n",
    "def palabra_más_probable(pulsaciones):\n",
    "    palabra_más_probable = ''\n",
    "    mayor_log_probabilidad = float('-inf')\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        bigramas_palabra = bigrams(palabra,\n",
    "                                   pad_left=True, left_pad_symbol='<s>',\n",
    "                                   pad_right=True, right_pad_symbol='</s>')\n",
    "        log_probabilidad_palabra = sum(modelo_bigrama_letras.logscore(bigrama[-1], context=bigrama[:-1])\n",
    "                                       for bigrama in bigramas_palabra)\n",
    "        if log_probabilidad_palabra > mayor_log_probabilidad:\n",
    "            palabra_más_probable = palabra\n",
    "            mayor_log_probabilidad = log_probabilidad_palabra\n",
    "    return ''.join(palabra_más_probable)\n",
    "\n",
    "print(palabra_más_probable(codifica_palabra('grito')))\n",
    "\n",
    "def palabras_más_probables(pulsaciones, n):\n",
    "    palabras = {}\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        bigramas_palabra = bigrams(palabra,\n",
    "                                   pad_left=True, left_pad_symbol='<s>',\n",
    "                                   pad_right=True, right_pad_symbol='</s>')\n",
    "        log_probabilidad_palabra = sum(modelo_bigrama_letras.logscore(bigrama[-1], context=bigrama[:-1])\n",
    "                                       for bigrama in bigramas_palabra)\n",
    "        palabras[''.join(palabra)] = log_probabilidad_palabra\n",
    "    return sorted(palabras, key=palabras.get, reverse=True)[:n]\n",
    "\n",
    "print(palabras_más_probables(codifica_palabra('grito'), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_trigrama_letras = MLE(3, vocabulary=vocabulario_letras)\n",
    "trigramas_letras = (trigrams(list(palabra),\n",
    "                             pad_left=True, left_pad_symbol='<s>',\n",
    "                             pad_right=True, right_pad_symbol='</s>')\n",
    "                    for palabra in corpus_entrenamiento.words())\n",
    "modelo_trigrama_letras.fit(trigramas_letras)\n",
    "\n",
    "def letras_más_probables(pulsaciones):\n",
    "    letras = []\n",
    "    letras_anteriores = ['<s>', '<s>']\n",
    "    for pulsación in pulsaciones:\n",
    "        bloque = bloques_de_letras[pulsación]\n",
    "        letra_más_probable = max(bloque,\n",
    "                                 key=lambda letra: modelo_trigrama_letras.score(letra, context=letras_anteriores))\n",
    "        letras.append(letra_más_probable)\n",
    "        letras_anteriores.append(letra_más_probable)\n",
    "        letras_anteriores.pop(0)\n",
    "    return ''.join(letras)\n",
    "\n",
    "print(letras_más_probables(codifica_palabra('grito')))\n",
    "\n",
    "def palabra_más_probable(pulsaciones):\n",
    "    palabra_más_probable = ''\n",
    "    mayor_log_probabilidad = float('-inf')\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        trigramas_palabra = trigrams(palabra,\n",
    "                                     pad_left=True, left_pad_symbol='<s>',\n",
    "                                     pad_right=True, right_pad_symbol='</s>')\n",
    "        log_probabilidad_palabra = sum(modelo_trigrama_letras.logscore(trigrama[-1], context=trigrama[:-1])\n",
    "                                       for trigrama in trigramas_palabra)\n",
    "        if log_probabilidad_palabra > mayor_log_probabilidad:\n",
    "            palabra_más_probable = palabra\n",
    "            mayor_log_probabilidad = log_probabilidad_palabra\n",
    "    return ''.join(palabra_más_probable)\n",
    "\n",
    "print(palabra_más_probable(codifica_palabra('grito')))\n",
    "\n",
    "def palabras_más_probables(pulsaciones, n):\n",
    "    palabras = {}\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        trigramas_palabra = trigrams(palabra,\n",
    "                                     pad_left=True, left_pad_symbol='<s>',\n",
    "                                     pad_right=True, right_pad_symbol='</s>')\n",
    "        log_probabilidad_palabra = sum(modelo_trigrama_letras.logscore(trigrama[-1], context=trigrama[:-1])\n",
    "                                       for trigrama in trigramas_palabra)\n",
    "        palabras[''.join(palabra)] = log_probabilidad_palabra\n",
    "    return sorted(palabras, key=palabras.get, reverse=True)[:n]\n",
    "\n",
    "print(palabras_más_probables(codifica_palabra('grito'), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de texto a partir de modelos de $n$-gramas de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos de $n$-gramas de letras no parecen proporcionar resultados satisfactorios. En esta segunda parte de la práctica se pretende construir un sistema de predicción de texto basado en un modelo de lenguaje construido a partir de modelos de $n$-gramas de palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso el vocabulario estará formado por todas las palabras identificadas en el corpus. Es posible establecer una cantidad mínima de ocurrencias para que una palabra se incluya o no en el vocabulario. Los símbolos de inicio y fin de secuencia delimitarán, en su caso, secuencias de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio_frase = '<s>'\n",
    "fin_frase = '</s>'\n",
    "# Cada palabra del vocabulario debe ocurrir al menos 5 veces en el corpus\n",
    "vocabulario_palabras = Vocabulary(corpus_entrenamiento.words(), unk_cutoff=5)\n",
    "vocabulario_palabras.update([inicio_frase, fin_frase])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 6**: construir un modelo de unigramas de palabras entrenado con el corpus proporcionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_unigrama_palabras = MLE(1, vocabulary=vocabulario_palabras)\n",
    "unigramas_palabras = (ngrams(frase, n=1)\n",
    "                      for frase in corpus_entrenamiento.sents())\n",
    "modelo_unigrama_palabras.fit(unigramas_palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 7**: definir una función que reciba una cadena de pulsaciones de teclas y una lista de palabras anteriores como contexto (en el caso del modelo unigrama el contexto será la lista vacía) y devuelva la cadena más probable compatible con esas pulsaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabra_más_probable(pulsaciones, contexto):\n",
    "    palabra_más_probable = ''\n",
    "    mayor_probabilidad = float('-inf')\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        palabra = ''.join(palabra)\n",
    "        if palabra in vocabulario_palabras:\n",
    "            probabilidad_palabra = modelo_unigrama_palabras.score(palabra)\n",
    "            if probabilidad_palabra > mayor_probabilidad:\n",
    "                palabra_más_probable = palabra\n",
    "                mayor_probabilidad = probabilidad_palabra\n",
    "    return palabra_más_probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabra_más_probable(codifica_palabra('grito'), contexto=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 8**: definir una función que reciba una cadena de pulsaciones de teclas, una lista de palabras anteriores como contexto y una cantidad $n$ de palabras y devuelva las $n$ cadenas más probables compatibles con esas pulsaciones, en orden decreciente de probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabras_más_probables(pulsaciones, contexto, n):\n",
    "    palabras = {}\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        palabra = ''.join(palabra)\n",
    "        if palabra in vocabulario_palabras:\n",
    "            probabilidad_palabra = modelo_unigrama_palabras.score(palabra)\n",
    "            palabras[palabra] = probabilidad_palabra\n",
    "    return sorted(palabras, key=palabras.get, reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_más_probables(codifica_palabra('grito'), contexto=[], n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 9**: repite los ejercicios 6 a 8 anteriores, pero construyendo modelos bigramas y trigramas de palabras, en lugar de un modelo unigrama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_bigrama_palabras = MLE(2, vocabulary=vocabulario_palabras)\n",
    "bigramas_palabras = (bigrams(frase,\n",
    "                             pad_left=True, left_pad_symbol='<s>',\n",
    "                             pad_right=True, right_pad_symbol='</s>')\n",
    "                    for frase in corpus_entrenamiento.sents())\n",
    "modelo_bigrama_palabras.fit(bigramas_palabras)\n",
    "\n",
    "def palabra_más_probable(pulsaciones, contexto):\n",
    "    palabra_más_probable = ''\n",
    "    mayor_probabilidad = float('-inf')\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        palabra = ''.join(palabra)\n",
    "        if palabra in vocabulario_palabras:\n",
    "            probabilidad_palabra = modelo_bigrama_palabras.score(palabra, context=contexto)\n",
    "            if probabilidad_palabra > mayor_probabilidad:\n",
    "                palabra_más_probable = palabra\n",
    "                mayor_probabilidad = probabilidad_palabra\n",
    "    return palabra_más_probable\n",
    "\n",
    "print(palabra_más_probable(codifica_palabra('grito'), contexto=['un']))\n",
    "\n",
    "def palabras_más_probables(pulsaciones, contexto, n):\n",
    "    palabras = {}\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        palabra = ''.join(palabra)\n",
    "        if palabra in vocabulario_palabras:\n",
    "            probabilidad_palabra = modelo_bigrama_palabras.score(palabra, context=contexto)\n",
    "            palabras[palabra] = probabilidad_palabra\n",
    "    return sorted(palabras, key=palabras.get, reverse=True)[:n]\n",
    "\n",
    "print(palabras_más_probables(codifica_palabra('grito'), contexto=['un'], n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_trigrama_palabras = MLE(3, vocabulary=vocabulario_palabras)\n",
    "trigramas_palabras = (trigrams(frase,\n",
    "                               pad_left=True, left_pad_symbol='<s>',\n",
    "                               pad_right=True, right_pad_symbol='</s>')\n",
    "                      for frase in corpus_entrenamiento.sents())\n",
    "modelo_trigrama_palabras.fit(trigramas_palabras)\n",
    "\n",
    "def palabra_más_probable(pulsaciones, contexto):\n",
    "    palabra_más_probable = ''\n",
    "    mayor_probabilidad = float('-inf')\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        palabra = ''.join(palabra)\n",
    "        if palabra in vocabulario_palabras:\n",
    "            probabilidad_palabra = modelo_trigrama_palabras.score(palabra, context=contexto)\n",
    "            if probabilidad_palabra > mayor_probabilidad:\n",
    "                palabra_más_probable = palabra\n",
    "                mayor_probabilidad = probabilidad_palabra\n",
    "    return palabra_más_probable\n",
    "\n",
    "print(palabra_más_probable(codifica_palabra('grito'), contexto=['un']))\n",
    "\n",
    "def palabras_más_probables(pulsaciones, contexto, n):\n",
    "    palabras = {}\n",
    "    bloques = (bloques_de_letras[pulsación] for pulsación in pulsaciones)\n",
    "    for palabra in itertools.product(*bloques):\n",
    "        palabra = ''.join(palabra)\n",
    "        if palabra in vocabulario_palabras:\n",
    "            probabilidad_palabra = modelo_trigrama_palabras.score(palabra, context=contexto)\n",
    "            palabras[palabra] = probabilidad_palabra\n",
    "    return sorted(palabras, key=palabras.get, reverse=True)[:n]\n",
    "\n",
    "print(palabras_más_probables(codifica_palabra('grito'), contexto=['un'], n=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "Solución.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
